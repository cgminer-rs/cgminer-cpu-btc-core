//! # å¹¶å‘ä¼˜åŒ–æ¨¡å— - æ— é”æ•°æ®ç»“æ„å’Œé«˜æ€§èƒ½å¹¶å‘
//!
//! æœ¬æ¨¡å—å®ç°äº†é«˜æ€§èƒ½çš„æ— é”å¹¶å‘æ•°æ®ç»“æ„ï¼Œä¸“é—¨ç”¨äºæ¶ˆé™¤CPUæŒ–çŸ¿ä¸­çš„é”ç«äº‰ç“¶é¢ˆã€‚
//! é€šè¿‡ä½¿ç”¨åŸå­æ“ä½œå’Œæ— é”é˜Ÿåˆ—ï¼Œæ˜¾è‘—æå‡å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚
//!
//! ## ğŸš€ æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯
//!
//! ### æ— é”å·¥ä½œé˜Ÿåˆ— ([`LockFreeWorkQueue`])
//! - âš¡ åŸºäºcrossbeamçš„æ— é”é˜Ÿåˆ—å®ç°
//! - âš¡ éé˜»å¡çš„å·¥ä½œå…¥é˜Ÿ/å‡ºé˜Ÿæ“ä½œ
//! - âš¡ æ”¯æŒå·¥ä½œç‰ˆæœ¬ç®¡ç†å’Œè¿‡æœŸæ¸…ç†
//! - âš¡ è¯¦ç»†çš„é˜Ÿåˆ—ç»Ÿè®¡å’Œç›‘æ§
//!
//! ### åŸå­ç»Ÿè®¡ç®¡ç†å™¨ ([`AtomicStatsManager`])
//! - ğŸ“Š å¤šè®¾å¤‡ç»Ÿè®¡ä¿¡æ¯èšåˆ
//! - ğŸ“Š åå°å¼‚æ­¥ç»Ÿè®¡æ›´æ–°
//! - ğŸ“Š å…¨å±€å’Œè®¾å¤‡çº§åˆ«çš„ç»Ÿè®¡åˆ†ç¦»
//! - ğŸ“Š å¯é…ç½®çš„æ›´æ–°é—´éš”
//!
//! ## ğŸ¯ æ€§èƒ½æå‡æ•ˆæœ
//!
//! | ä¼˜åŒ–é¡¹ç›® | ä¼ ç»Ÿæ–¹æ¡ˆ | æ— é”æ–¹æ¡ˆ | æ€§èƒ½æå‡ |
//! |----------|----------|----------|----------|
//! | å·¥ä½œé˜Ÿåˆ— | `Mutex<VecDeque>` | `ArrayQueue` | ~3-5x |
//! | ç»Ÿè®¡æ›´æ–° | `RwLock<Stats>` | `AtomicStats` | ~2-4x |
//! | æ‰¹é‡æ“ä½œ | é€ä¸ªæ›´æ–° | æ‰¹é‡æäº¤ | ~1.5-3x |
//!
//! ## ğŸ“¦ ä¸»è¦ç»„ä»¶
//!
//! ### [`LockFreeWorkQueue`] - æ— é”å·¥ä½œé˜Ÿåˆ—
//! ```text
//! ç‰¹æ€§:
//! â”œâ”€â”€ æœ‰ç•Œå¾…å¤„ç†é˜Ÿåˆ— (é˜²æ­¢å†…å­˜æº¢å‡º)
//! â”œâ”€â”€ æ— ç•Œå®Œæˆé˜Ÿåˆ— (åŠæ—¶å¤„ç†ç»“æœ)
//! â”œâ”€â”€ åŸå­è®¡æ•°å™¨ (æ´»è·ƒå·¥ä½œç»Ÿè®¡)
//! â”œâ”€â”€ ç‰ˆæœ¬ç®¡ç† (å¿«é€Ÿè¿‡æœŸæ£€æµ‹)
//! â””â”€â”€ æ€§èƒ½ç›‘æ§ (é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯)
//! ```
//!
//! ### [`AtomicStatsManager`] - åŸå­ç»Ÿè®¡ç®¡ç†
//! ```text
//! åŠŸèƒ½:
//! â”œâ”€â”€ è®¾å¤‡æ³¨å†Œå’Œç®¡ç†
//! â”œâ”€â”€ å…¨å±€ç»Ÿè®¡èšåˆ
//! â”œâ”€â”€ åå°å®šæ—¶æ›´æ–°
//! â”œâ”€â”€ æ‰¹é‡é‡ç½®æ“ä½œ
//! â””â”€â”€ ç®¡ç†å™¨çŠ¶æ€æŸ¥è¯¢
//! ```
//!
//! ### [`WorkQueueStats`] - é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
//! - ğŸ“ˆ å¾…å¤„ç†/æ´»è·ƒ/å®Œæˆå·¥ä½œæ•°é‡
//! - ğŸ“ˆ æ€»å…¥é˜Ÿ/å‡ºé˜Ÿè®¡æ•°
//! - ğŸ“ˆ é˜Ÿåˆ—æ»¡è½½æ¬¡æ•°ç»Ÿè®¡
//! - ğŸ“ˆ å½“å‰å·¥ä½œç‰ˆæœ¬å·
//!
//! ## ğŸ”„ ä½¿ç”¨æ¨¡å¼
//!
//! ### åŸºæœ¬å·¥ä½œé˜Ÿåˆ—ä½¿ç”¨
//! ```rust
//! // åˆ›å»ºæ— é”å·¥ä½œé˜Ÿåˆ—
//! let queue = LockFreeWorkQueue::new(1000);
//!
//! // éé˜»å¡å…¥é˜Ÿ
//! if let Err(work) = queue.enqueue_work(work) {
//!     // é˜Ÿåˆ—å·²æ»¡ï¼Œå¤„ç†æº¢å‡º
//! }
//!
//! // éé˜»å¡å‡ºé˜Ÿ
//! if let Some(work) = queue.dequeue_work() {
//!     // å¤„ç†å·¥ä½œ
//! }
//! ```
//!
//! ### ç»Ÿè®¡ç®¡ç†å™¨ä½¿ç”¨
//! ```rust
//! // åˆ›å»ºç»Ÿè®¡ç®¡ç†å™¨
//! let manager = AtomicStatsManager::new(100); // 100msæ›´æ–°é—´éš”
//!
//! // æ³¨å†Œè®¾å¤‡
//! let stats = manager.register_device(device_id);
//!
//! // å¯åŠ¨åå°èšåˆ
//! let handle = manager.start_background_aggregation().await;
//! ```
//!
//! ## âš™ï¸ è®¾è®¡åŸåˆ™
//!
//! 1. **æ— é”ä¼˜å…ˆ**: ä½¿ç”¨åŸå­æ“ä½œæ›¿ä»£é”æœºåˆ¶
//! 2. **æ‰¹é‡å¤„ç†**: å‡å°‘é«˜é¢‘æ“ä½œçš„ç³»ç»Ÿå¼€é”€
//! 3. **å†…å­˜æ•ˆç‡**: åˆç†çš„é˜Ÿåˆ—å¤§å°å’Œå†…å­˜ç®¡ç†
//! 4. **å¯è§‚æµ‹æ€§**: è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡å’Œç›‘æ§
//! 5. **å®¹é”™æ€§**: ä¼˜é›…å¤„ç†é˜Ÿåˆ—æ»¡è½½å’Œå¼‚å¸¸æƒ…å†µ

use cgminer_core::{Work, MiningResult, DeviceStats};
use crate::device::AtomicStats;
use crossbeam::queue::{ArrayQueue, SegQueue};
use std::sync::{Arc, atomic::{AtomicUsize, Ordering}};
use std::collections::HashMap;
use std::time::{Duration, Instant};
use tokio::time;
use tracing::{debug, info, warn};

/// æ— é”å·¥ä½œé˜Ÿåˆ— - æ¶ˆé™¤å·¥ä½œåˆ†å‘ä¸­çš„é”ç«äº‰
/// ä½¿ç”¨crossbeamçš„æ— é”é˜Ÿåˆ—æ›¿æ¢ä¼ ç»Ÿçš„Mutex<VecDeque>
/// ä½¿ç”¨Arc<Work>å®ç°é›¶æ‹·è´
#[derive(Debug)]
pub struct LockFreeWorkQueue {
    // å¾…å¤„ç†å·¥ä½œé˜Ÿåˆ—ï¼ˆæœ‰ç•Œé˜Ÿåˆ—ï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºï¼‰- ä½¿ç”¨Arc<Work>å®ç°é›¶æ‹·è´
    pending_work: Arc<ArrayQueue<Arc<Work>>>,
    // å·²å®Œæˆå·¥ä½œé˜Ÿåˆ—ï¼ˆæ— ç•Œé˜Ÿåˆ—ï¼Œç»“æœéœ€è¦åŠæ—¶å¤„ç†ï¼‰
    completed_work: Arc<SegQueue<MiningResult>>,
    // æ´»è·ƒå·¥ä½œè®¡æ•°å™¨
    active_work_count: Arc<AtomicUsize>,
    // é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
    total_enqueued: Arc<AtomicUsize>,
    total_dequeued: Arc<AtomicUsize>,
    queue_full_count: Arc<AtomicUsize>,
    // å·¥ä½œç‰ˆæœ¬ç®¡ç† - ç”¨äºå¿«é€Ÿè¿‡æœŸæ£€æµ‹
    current_work_version: Arc<AtomicUsize>,
    max_queue_size: usize,
}

impl LockFreeWorkQueue {
    /// åˆ›å»ºæ–°çš„æ— é”å·¥ä½œé˜Ÿåˆ—
    pub fn new(max_queue_size: usize) -> Self {
        Self {
            pending_work: Arc::new(ArrayQueue::new(max_queue_size)),
            completed_work: Arc::new(SegQueue::new()),
            active_work_count: Arc::new(AtomicUsize::new(0)),
            total_enqueued: Arc::new(AtomicUsize::new(0)),
            total_dequeued: Arc::new(AtomicUsize::new(0)),
            queue_full_count: Arc::new(AtomicUsize::new(0)),
            current_work_version: Arc::new(AtomicUsize::new(0)),
            max_queue_size,
        }
    }

    /// æ— é”å…¥é˜Ÿå·¥ä½œ - éé˜»å¡æ“ä½œï¼Œä½¿ç”¨Arc<Work>å®ç°é›¶æ‹·è´
    pub fn enqueue_work(&self, work: Arc<Work>) -> Result<(), Arc<Work>> {
        match self.pending_work.push(work) {
            Ok(()) => {
                self.active_work_count.fetch_add(1, Ordering::Relaxed);
                self.total_enqueued.fetch_add(1, Ordering::Relaxed);
                debug!("å·¥ä½œæˆåŠŸå…¥é˜Ÿï¼Œå½“å‰é˜Ÿåˆ—é•¿åº¦: {}", self.active_work_count.load(Ordering::Relaxed));
                Ok(())
            }
            Err(work) => {
                self.queue_full_count.fetch_add(1, Ordering::Relaxed);
                warn!("å·¥ä½œé˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒå·¥ä½œ");
                Err(work)
            }
        }
    }

    /// æ— é”å‡ºé˜Ÿå·¥ä½œ - éé˜»å¡æ“ä½œï¼Œè¿”å›Arc<Work>å®ç°é›¶æ‹·è´
    pub fn dequeue_work(&self) -> Option<Arc<Work>> {
        match self.pending_work.pop() {
            Some(work) => {
                self.total_dequeued.fetch_add(1, Ordering::Relaxed);
                debug!("å·¥ä½œæˆåŠŸå‡ºé˜Ÿ");
                Some(work)
            }
            None => None,
        }
    }

    /// æäº¤å®Œæˆçš„å·¥ä½œç»“æœ
    pub fn submit_result(&self, result: MiningResult) {
        self.completed_work.push(result);
        self.active_work_count.fetch_sub(1, Ordering::Relaxed);
        debug!("æŒ–çŸ¿ç»“æœå·²æäº¤ï¼Œå½“å‰æ´»è·ƒå·¥ä½œæ•°: {}", self.active_work_count.load(Ordering::Relaxed));
    }

    /// è·å–å®Œæˆçš„å·¥ä½œç»“æœ
    pub fn get_result(&self) -> Option<MiningResult> {
        self.completed_work.pop()
    }

    /// æ‰¹é‡è·å–å®Œæˆçš„å·¥ä½œç»“æœ
    pub fn get_results(&self, max_count: usize) -> Vec<MiningResult> {
        let mut results = Vec::with_capacity(max_count);

        for _ in 0..max_count {
            if let Some(result) = self.completed_work.pop() {
                results.push(result);
            } else {
                break;
            }
        }

        results
    }

    /// æ›´æ–°å·¥ä½œç‰ˆæœ¬ - ç”¨äºå¿«é€Ÿæ£€æµ‹è¿‡æœŸå·¥ä½œ
    pub fn update_work_version(&self) -> usize {
        self.current_work_version.fetch_add(1, Ordering::Relaxed)
    }

    /// è·å–å½“å‰å·¥ä½œç‰ˆæœ¬
    pub fn current_version(&self) -> usize {
        self.current_work_version.load(Ordering::Relaxed)
    }

    /// æ¸…ç©ºæ‰€æœ‰è¿‡æœŸå·¥ä½œ
    pub fn clear_stale_work(&self, valid_version: usize) -> usize {
        let mut cleared_count = 0;
        let mut temp_works = Vec::new();

        // å–å‡ºæ‰€æœ‰å·¥ä½œè¿›è¡Œç‰ˆæœ¬æ£€æŸ¥
        while let Some(work) = self.pending_work.pop() {
            if work.version as usize >= valid_version {
                temp_works.push(work);
            } else {
                cleared_count += 1;
                self.active_work_count.fetch_sub(1, Ordering::Relaxed);
            }
        }

        // å°†æœ‰æ•ˆå·¥ä½œé‡æ–°å…¥é˜Ÿ
        for work in temp_works {
            if let Err(_) = self.pending_work.push(work) {
                // é˜Ÿåˆ—æ»¡äº†ï¼Œè¿™äº›å·¥ä½œä¼šè¢«ä¸¢å¼ƒ
                self.active_work_count.fetch_sub(1, Ordering::Relaxed);
                cleared_count += 1;
            }
        }

        if cleared_count > 0 {
            info!("æ¸…ç†è¿‡æœŸå·¥ä½œæ•°é‡: {}", cleared_count);
        }

        cleared_count
    }

    /// è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> WorkQueueStats {
        WorkQueueStats {
            pending_count: self.pending_work.len(),
            active_count: self.active_work_count.load(Ordering::Relaxed),
            completed_count: self.completed_work.len(),
            total_enqueued: self.total_enqueued.load(Ordering::Relaxed),
            total_dequeued: self.total_dequeued.load(Ordering::Relaxed),
            queue_full_count: self.queue_full_count.load(Ordering::Relaxed),
            current_version: self.current_work_version.load(Ordering::Relaxed),
            max_queue_size: self.max_queue_size,
        }
    }

    /// æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦æ¥è¿‘æ»¡è½½
    pub fn is_nearly_full(&self, threshold: f32) -> bool {
        let current_size = self.pending_work.len();
        let capacity = self.max_queue_size;
        current_size as f32 / capacity as f32 > threshold
    }
}

/// å·¥ä½œé˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct WorkQueueStats {
    pub pending_count: usize,
    pub active_count: usize,
    pub completed_count: usize,
    pub total_enqueued: usize,
    pub total_dequeued: usize,
    pub queue_full_count: usize,
    pub current_version: usize,
    pub max_queue_size: usize,
}

/// åŸå­ç»Ÿè®¡ç®¡ç†å™¨ - ç®¡ç†å¤šä¸ªè®¾å¤‡çš„åŸå­ç»Ÿè®¡
#[derive(Debug)]
pub struct AtomicStatsManager {
    device_stats: Arc<HashMap<u32, Arc<AtomicStats>>>,
    global_stats: Arc<AtomicStats>,
    update_interval: Duration,
    last_batch_update: Arc<std::sync::Mutex<Instant>>,
}

impl AtomicStatsManager {
    /// åˆ›å»ºæ–°çš„åŸå­ç»Ÿè®¡ç®¡ç†å™¨
    pub fn new(update_interval_ms: u64) -> Self {
        Self {
            device_stats: Arc::new(HashMap::new()),
            global_stats: Arc::new(AtomicStats::new(0)), // å…¨å±€ç»Ÿè®¡ä½¿ç”¨è®¾å¤‡ID 0
            update_interval: Duration::from_millis(update_interval_ms),
            last_batch_update: Arc::new(std::sync::Mutex::new(Instant::now())),
        }
    }

    /// æ³¨å†Œè®¾å¤‡ç»Ÿè®¡
    pub fn register_device(&mut self, device_id: u32) -> Arc<AtomicStats> {
        let stats = Arc::new(AtomicStats::new(device_id));
        Arc::get_mut(&mut self.device_stats)
            .unwrap()
            .insert(device_id, stats.clone());
        info!("æ³¨å†Œè®¾å¤‡ {} çš„åŸå­ç»Ÿè®¡", device_id);
        stats
    }

    /// è·å–è®¾å¤‡ç»Ÿè®¡
    pub fn get_device_stats(&self, device_id: u32) -> Option<Arc<AtomicStats>> {
        self.device_stats.get(&device_id).cloned()
    }

    /// è·å–å…¨å±€ç»Ÿè®¡
    pub fn get_global_stats(&self) -> Arc<AtomicStats> {
        self.global_stats.clone()
    }

    /// èšåˆæ‰€æœ‰è®¾å¤‡çš„ç»Ÿè®¡ä¿¡æ¯
    pub fn aggregate_stats(&self) -> DeviceStats {
        let mut total_hashes = 0u64;
        let mut total_accepted = 0u64;
        let mut total_rejected = 0u64;
        let mut total_errors = 0u64;
        let mut total_hashrate = 0.0f64;
        let device_count = self.device_stats.len();

        for stats in self.device_stats.values() {
            // è·å–åŸå§‹æ•°æ®å¹¶è®¡ç®—ç®—åŠ›
            let (device_hashes, start_time, last_update) = stats.get_raw_stats();
            let current_time = std::time::SystemTime::now()
                .duration_since(std::time::SystemTime::UNIX_EPOCH)
                .unwrap_or_default()
                .as_nanos() as u64;

            // è®¡ç®—è®¾å¤‡ç®—åŠ›
            let total_elapsed = (current_time - start_time) as f64 / 1_000_000_000.0;
            let device_hashrate = if total_elapsed > 0.0 {
                device_hashes as f64 / total_elapsed
            } else {
                0.0
            };

            let device_stats = stats.to_device_stats_with_hashrate(device_hashrate, device_hashrate);
            total_hashes += device_stats.total_hashes;
            total_accepted += device_stats.accepted_work;
            total_rejected += device_stats.rejected_work;
            total_errors += device_stats.hardware_errors;
            total_hashrate += device_stats.current_hashrate.hashes_per_second;
        }

        // æ›´æ–°å…¨å±€ç»Ÿè®¡
        let global = &self.global_stats;
        global.total_hashes.store(total_hashes, Ordering::Relaxed);
        global.accepted_work.store(total_accepted, Ordering::Relaxed);
        global.rejected_work.store(total_rejected, Ordering::Relaxed);
        global.hardware_errors.store(total_errors, Ordering::Relaxed);
        global.last_hashrate.store(total_hashrate.to_bits(), Ordering::Relaxed);

        // è®¡ç®—å¹³å‡å“ˆå¸Œç‡
        let avg_hashrate = if device_count > 0 {
            total_hashrate / device_count as f64
        } else {
            0.0
        };
        global.average_hashrate.store(avg_hashrate.to_bits(), Ordering::Relaxed);

        // è®¡ç®—å…¨å±€ç®—åŠ›å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯
        global.to_device_stats_with_hashrate(total_hashrate, avg_hashrate)
    }

    /// å¯åŠ¨åå°ç»Ÿè®¡èšåˆä»»åŠ¡
    pub async fn start_background_aggregation(self: Arc<Self>) -> tokio::task::JoinHandle<()> {
        let manager = self.clone();

        tokio::spawn(async move {
            let mut interval = time::interval(manager.update_interval);

            loop {
                interval.tick().await;
                let start_time = Instant::now();

                // æ‰§è¡Œç»Ÿè®¡èšåˆ
                let global_stats = manager.aggregate_stats();
                let elapsed = start_time.elapsed();

                debug!(
                    "ç»Ÿè®¡èšåˆå®Œæˆ: æ€»å“ˆå¸Œ={}, æ¥å—={}, æ‹’ç»={}, é”™è¯¯={}, è€—æ—¶={:?}",
                    global_stats.total_hashes,
                    global_stats.accepted_work,
                    global_stats.rejected_work,
                    global_stats.hardware_errors,
                    elapsed
                );

                // å¦‚æœèšåˆè€—æ—¶è¿‡é•¿ï¼Œå‘å‡ºè­¦å‘Š
                if elapsed > manager.update_interval / 2 {
                    warn!("ç»Ÿè®¡èšåˆè€—æ—¶è¿‡é•¿: {:?}, å¯èƒ½å½±å“æ€§èƒ½", elapsed);
                }
            }
        })
    }

    /// é‡ç½®æ‰€æœ‰è®¾å¤‡ç»Ÿè®¡
    pub fn reset_all_stats(&self) {
        for stats in self.device_stats.values() {
            stats.reset();
        }
        self.global_stats.reset();
        info!("å·²é‡ç½®æ‰€æœ‰è®¾å¤‡ç»Ÿè®¡");
    }

    /// è·å–ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
    pub fn get_manager_stats(&self) -> ManagerStats {
        ManagerStats {
            device_count: self.device_stats.len(),
            update_interval_ms: self.update_interval.as_millis() as u64,
            last_update: self.last_batch_update.lock().unwrap().elapsed(),
        }
    }
}

/// ç»Ÿè®¡ç®¡ç†å™¨ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct ManagerStats {
    pub device_count: usize,
    pub update_interval_ms: u64,
    pub last_update: Duration,
}

/// æ‰¹é‡ç»Ÿè®¡æ›´æ–°å™¨ï¼ˆä»device.rsç§»åŠ¨åˆ°è¿™é‡Œï¼‰
pub use crate::device::BatchStatsUpdater;

#[cfg(test)]
mod tests {
    use super::*;
    use cgminer_core::Work;
    use std::thread;


    #[test]
    fn test_lock_free_work_queue() {
        let queue = LockFreeWorkQueue::new(10);

        // æµ‹è¯•å·¥ä½œå…¥é˜Ÿå’Œå‡ºé˜Ÿ
        let work = Arc::new(Work::new("test_job_1".to_string(), [0u8; 32], [0u8; 80], 1.0));
        assert!(queue.enqueue_work(work.clone()).is_ok());

        let dequeued = queue.dequeue_work();
        assert!(dequeued.is_some());
        assert_eq!(dequeued.unwrap().id, work.id);

        // æµ‹è¯•ç©ºé˜Ÿåˆ—
        assert!(queue.dequeue_work().is_none());
    }

    #[test]
    fn test_queue_full_handling() {
        let queue = LockFreeWorkQueue::new(2);

        // å¡«æ»¡é˜Ÿåˆ—
        let work1 = Arc::new(Work::new("test_job_1".to_string(), [0u8; 32], [0u8; 80], 1.0));
        let work2 = Arc::new(Work::new("test_job_2".to_string(), [0u8; 32], [0u8; 80], 1.0));
        let work3 = Arc::new(Work::new("test_job_3".to_string(), [0u8; 32], [0u8; 80], 1.0));

        assert!(queue.enqueue_work(work1).is_ok());
        assert!(queue.enqueue_work(work2).is_ok());
        assert!(queue.enqueue_work(work3).is_err()); // åº”è¯¥å¤±è´¥

        let stats = queue.get_stats();
        assert_eq!(stats.queue_full_count, 1);
    }

    #[tokio::test]
    async fn test_atomic_stats_manager() {
        let mut manager = AtomicStatsManager::new(100);

        // æ³¨å†Œè®¾å¤‡
        let stats1 = manager.register_device(1);
        let stats2 = manager.register_device(2);

        // æ›´æ–°ç»Ÿè®¡ - è®°å½•å“ˆå¸Œæ•°è€Œä¸æ˜¯ç®—åŠ›
        stats1.record_hashes(1000);
        stats2.record_hashes(2000);

        // èšåˆç»Ÿè®¡
        let global_stats = manager.aggregate_stats();
        assert_eq!(global_stats.total_hashes, 3000);
    }

    #[test]
    fn test_concurrent_queue_access() {
        let queue = Arc::new(LockFreeWorkQueue::new(1000));
        let mut handles = vec![];

        // ç”Ÿäº§è€…çº¿ç¨‹
        for i in 0..4 {
            let queue_clone = queue.clone();
            let handle = thread::spawn(move || {
                for j in 0..100 {
                    let work = Arc::new(Work::new(format!("job_{}_{}", i, j), [0u8; 32], [0u8; 80], 1.0));
                    let _ = queue_clone.enqueue_work(work);
                }
            });
            handles.push(handle);
        }

        // æ¶ˆè´¹è€…çº¿ç¨‹
        for _ in 0..2 {
            let queue_clone = queue.clone();
            let handle = thread::spawn(move || {
                let mut consumed = 0;
                while consumed < 200 {
                    if let Some(_work) = queue_clone.dequeue_work() {
                        consumed += 1;
                    } else {
                        thread::yield_now();
                    }
                }
            });
            handles.push(handle);
        }

        // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for handle in handles {
            handle.join().unwrap();
        }

        let stats = queue.get_stats();
        assert_eq!(stats.total_enqueued, 400);
        assert_eq!(stats.total_dequeued, 400);
    }
}
